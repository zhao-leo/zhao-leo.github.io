<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on April Diary</title>
    <link>https://www.zhaocloud.work/tags/ai/</link>
    <description>Recent content in AI on April Diary</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <copyright>April. 本站遵循 CC-BY-NC 4.0 协议</copyright>
    <lastBuildDate>Tue, 27 May 2025 22:44:07 +0800</lastBuildDate>
    <atom:link href="https://www.zhaocloud.work/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>一次失败的模型导出</title>
      <link>https://www.zhaocloud.work/posts/%E4%B8%80%E6%AC%A1%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%87%BA/</link>
      <pubDate>Tue, 27 May 2025 22:44:07 +0800</pubDate>
      <guid>https://www.zhaocloud.work/posts/%E4%B8%80%E6%AC%A1%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%87%BA/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;PS: 今天看网站才发现出BUG了（😅）&#xA;今天实现的部分是针对&lt;a href=&#34;https://github.com/opendatalab/PDF-Extract-Kit&#34;&gt;PDF-Extract-Kit&lt;/a&gt;的两个YOLO布局检测（公式检测）模型的。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;其主要逻辑仍然是我之前写过的&lt;a href=&#34;https://www.zhaocloud.work/posts/yolo-binding%E7%9A%84%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86/&#34;&gt;Yolo Binding的相关代码部分&lt;/a&gt;，但是不同的是这次我会使用静态链接库以提高可移植性。&lt;/p&gt;&#xA;&lt;p&gt;Thanks to &lt;code&gt;tch-rs&lt;/code&gt;，我们可以通过环境变量&lt;code&gt;LIBTORCH_STATIC=1&lt;/code&gt;来选择使用静态链接库(回看一下，这个也没搞定)&lt;/p&gt;&#xA;&lt;p&gt;下面需要解决的事情是将官方提供的pt模型转成jit模型提高速度和可移植性。&lt;/p&gt;&#xA;&lt;h2 id=&#34;doclayout-yolo&#34;&gt;&lt;a href=&#34;https://github.com/opendatalab/DocLayout-YOLO&#34;&gt;DocLayout-YOLO&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;首先，根据官网&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;说明，去&lt;a href=&#34;https://www.modelscope.cn/models/OpenDataLab/PDF-Extract-Kit-1.0&#34;&gt;modelscope&lt;/a&gt;/&lt;a href=&#34;https://huggingface.co/opendatalab/PDF-Extract-Kit-1.0&#34;&gt;huggingface&lt;/a&gt;上下载模型，随后下载包如下：&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-powershell&#34; data-lang=&#34;powershell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install &lt;span style=&#34;color:#ff5c57&#34;&gt;doclayout-yolo&lt;/span&gt; &lt;span style=&#34;color:#78787e&#34;&gt;# 安装官方提供的包&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install &lt;span style=&#34;color:#ff5c57&#34;&gt;huggingface-hub&lt;/span&gt; &lt;span style=&#34;color:#78787e&#34;&gt;# 没有这个无法运行&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install torch torchvision torchaudio --index-url https&lt;span style=&#34;color:#ff5c57&#34;&gt;:&lt;/span&gt;//download.pytorch.org/whl/cu128 &lt;span style=&#34;color:#78787e&#34;&gt;# 更新torch使其支持torch&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;检验安装&#34;&gt;检验安装&lt;/h3&gt;&#xA;&lt;p&gt;之后创建&lt;code&gt;test.py&lt;/code&gt;检验安装情况：&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff6ac1&#34;&gt;import&lt;/span&gt; cv2&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff6ac1&#34;&gt;from&lt;/span&gt; doclayout_yolo &lt;span style=&#34;color:#ff6ac1&#34;&gt;import&lt;/span&gt; YOLOv10&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#78787e&#34;&gt;# Load the pre-trained model&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model &lt;span style=&#34;color:#ff6ac1&#34;&gt;=&lt;/span&gt; YOLOv10(&lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;./models/Layout/YOLO/doclayout_yolo_ft.pt&amp;#34;&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#78787e&#34;&gt;# Perform prediction&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;det_res &lt;span style=&#34;color:#ff6ac1&#34;&gt;=&lt;/span&gt; model&lt;span style=&#34;color:#ff6ac1&#34;&gt;.&lt;/span&gt;predict(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;path/to/image&amp;#34;&lt;/span&gt;,   &lt;span style=&#34;color:#78787e&#34;&gt;# Image to predict&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    imgsz&lt;span style=&#34;color:#ff6ac1&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ff9f43&#34;&gt;1024&lt;/span&gt;,        &lt;span style=&#34;color:#78787e&#34;&gt;# Prediction image size&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    conf&lt;span style=&#34;color:#ff6ac1&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ff9f43&#34;&gt;0.2&lt;/span&gt;,          &lt;span style=&#34;color:#78787e&#34;&gt;# Confidence threshold&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    device&lt;span style=&#34;color:#ff6ac1&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;cuda:0&amp;#34;&lt;/span&gt;    &lt;span style=&#34;color:#78787e&#34;&gt;# Device to use (e.g., &amp;#39;cuda:0&amp;#39; or &amp;#39;cpu&amp;#39;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#78787e&#34;&gt;# Annotate and save the result&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;annotated_frame &lt;span style=&#34;color:#ff6ac1&#34;&gt;=&lt;/span&gt; det_res[&lt;span style=&#34;color:#ff9f43&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#ff6ac1&#34;&gt;.&lt;/span&gt;plot(pil&lt;span style=&#34;color:#ff6ac1&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ff6ac1&#34;&gt;True&lt;/span&gt;, line_width&lt;span style=&#34;color:#ff6ac1&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ff9f43&#34;&gt;5&lt;/span&gt;, font_size&lt;span style=&#34;color:#ff6ac1&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ff9f43&#34;&gt;20&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cv2&lt;span style=&#34;color:#ff6ac1&#34;&gt;.&lt;/span&gt;imwrite(&lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;result.jpg&amp;#34;&lt;/span&gt;, annotated_frame)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;看到终端输出类似即说明安装成功：&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
